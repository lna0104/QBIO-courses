value_2 = max(H[(i-1):1, j] + (1:(i-1)) * (-1))
value_3 = max(H[i, (j-1):1] + (1:(j-1)) * (-1))
H[i,j] = max(value_1, value_2, value_3, 0)
}
}
i=2
View(s)
j=9
aList[i]
bList[j]
bList
a = "CAGCGGC"
b = "CATCGGTC"
aList = strsplit(a, "")[[1]]
bList = strsplit(b, "")[[1]]
H = matrix(0, length(aList)+1, length(bList)+1)
s = matrix(0, length(aList), length(bList))
for (i in (2:nrow(H))){
for (j in (2: ncol(H))){
if (aList[i-1] == bList[j-1]){
s[i-1,j-1] = 2
}else {
s[i-1,j-1] = -2
}
value_1 = H[i-1,j-1] + s[i-1,j-1]
value_2 = max(H[(i-1):1, j] + (1:(i-1)) * (-1))
value_3 = max(H[i, (j-1):1] + (1:(j-1)) * (-1))
H[i,j] = max(value_1, value_2, value_3, 0)
}
}
View(H)
a = "CAGCGGC"
b = "CATCGGTC"
aList = strsplit(a, "")[[1]]
bList = strsplit(b, "")[[1]]
H = matrix(0, length(aList)+1, length(bList)+1)
for (i in (2:nrow(H))){
for (j in (2: ncol(H))){
if (aList[i-1] == bList[j-1]){
value_1 = H[i-1,j-1] + 2
}else {
value_1 = H[i-1,j-1] - 2
}
value_2 = max(H[(i-1):1, j] + (1:(i-1)) * (-1))
value_3 = max(H[i, (j-1):1] + (1:(j-1)) * (-1))
H[i,j] = max(value_1, value_2, value_3, 0L)
}
}
a = "CAGCGGC"
b = "CATCGGTC"
aList = strsplit(a, "")[[1]]
bList = strsplit(b, "")[[1]]
H = matrix(0, length(aList)+1, length(bList)+1)
a = "CAGCGGC"
b = "CATCGGTC"
aList = strsplit(a, "")[[1]]
bList = strsplit(b, "")[[1]]
H = matrix(0, length(aList)+1, length(bList)+1)
for (i in (2:nrow(H))){
for (j in (2: ncol(H))){
if (aList[i-1] == bList[j-1]){
value_1 = H[i-1,j-1] + 2
}else {
value_1 = H[i-1,j-1] - 2
}
value_2 = max(H[(i-1):1, j] + 1:(i-1) * (-1))
value_3 = max(H[i, (j-1):1] + 1:(j-1) * (-1))
H[i,j] = max(value_1, value_2, value_3, 0L)
}
}
a = "CAGCGGC"
b = "CATCGGTC"
aList = strsplit(a, "")[[1]]
bList = strsplit(b, "")[[1]]
H = matrix(0, length(aList)+1, length(bList)+1)
for (i in (2:nrow(H))){
for (j in (2: ncol(H))){
if (aList[i-1] == bList[j-1]){
value_1 = H[i-1,j-1] + 2
}else {
value_1 = H[i-1,j-1] - 2
}
value_2 = max(H[(i-1):1, j] + 1:(i-1) * (-10))
value_3 = max(H[i, (j-1):1] + 1:(j-1) * (-10))
H[i,j] = max(value_1, value_2, value_3, 0L)
}
}
algorithmSmithWaterman <- function(a, b, match_score, mismatch_score, gap_score){
aList = strsplit(a, "")[[1]]
bList = strsplit(b, "")[[1]]
H = matrix(0, length(aList)+1, length(bList)+1)
for (i in (2:nrow(H))){
for (j in (2: ncol(H))){
if (aList[i-1] == bList[j-1]){
value_1 = H[i-1,j-1] + match_score
}else {
value_1 = H[i-1,j-1] + mismatch_score
}
value_2 = max(H[(i-1):1, j] + 1:(i-1) * gap_score)
value_3 = max(H[i, (j-1):1] + 1:(j-1) * gap_score)
H[i,j] = max(value_1, value_2, value_3, 0L)
}
}
return (H)
}
result = algorithmSmithWaterman(a, b, 2, -2, -1)
View(result)
#4.1 Adding a continuous predictor variable ------
plot(d2$weight, d2$height, ylab = "Adult height (cm)", xlab = "Adult weight (kg)")
#3.2 A Gaussian model of height -----
library(tidyverse)
d<-read_csv("data/iKung_HeightWeight.csv")
#3.2 A Gaussian model of height -----
library(tidyverse)
d<-read_csv("Desktop/Courses/QBIO7005/data/iKung_HeightWeight.csv")
head(d)
tail(d)
str(d)
d2 <- d %>% filter(age>=18)
dim(d2)
hist(d2$height,breaks = 20,freq=F,ylim=c(0,0.1),main="Histogram of Adult Height",xlab="height (cm)")
meanGuess <- 155
sdGuess <-10
curve((1/(sdGuess * sqrt(2 * pi))) * exp(-0.5 * ((x - meanGuess) / sdGuess)^2),
add = TRUE, col = "red", lwd = 2)
meanHeight <- mean(d2$height, na.rm = T)
sdHeight <- sd(d2$height, na.rm = T)
#3.4 Fitting the model in R -----
# Using Ordinary Least Squares estimate
m1 <- lm(height~1,data=d2) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
summary(m1) # this provides a summary of the model fit, including parameter estimates.
str(summary(m1))
m1Summary = summary(m1)
m1Summary$coefficients
m1Summary$sigma
# Using maximum likelihood estimation
{
normalF <- function(parvec) {
# Log of likelihood of a normal distribution
# parvec[1] - mean
# parvec[2] - standard deviation
# x - set of observations. Should be initialized before MLE
sum ( -0.5* log(parvec[2]) - 0.5*(x - parvec[1])^2/parvec[2] )
}
x = c(1,2,3,4) # set of observations
normalF(c(1,1)) # log likelihood function value for given x and mu=sd=1
}
library(bbmle)
x <- na.omit(d2$height)
m <- mle2(x~dnorm(mean=mu,sd=sd),start=list(mu=145,sd=6),data=data.frame(x))
m
confint(m)
#4.1 Adding a continuous predictor variable ------
plot(d2$weight, d2$height, ylab = "Adult height (cm)", xlab = "Adult weight (kg)")
m1 <- lm(formula = height ~ weight, data=d2) # assign a linear model (lm) fit to the object 'm1'. We are modeling height as a function of weight. The `lm` command assumes a normally distributed data (i.e. that a normal likelihood is justified)
m1.sum <- summary(m1)
plot(d2$weight, d2$height, ylab = "Adult height (cm)", xlab = "Adult weight (kg)")
abline(lm(height ~ weight, data = d2), col = "red")
str(m1)
plot(m1$residuals ~ m1$fitted.values)
abline(h=0)
coef(m1)
newdat <- as.data.frame(59) # i.e. I would like to know the predicted height for an individual who weighs 59kg.
newdat
names(newdat) <- "weight"
predict(m1,newdata=newdat)
predict(m1,newdata=newdat,interval="p")
library(tidyverse)
setwd("~/Desktop/Courses/QBIO7005")
cocoaDat <- read_csv("data/cocoa_data_02.csv")
cocoaDat <- read_csv("data/cocoa_data_02.csv")
View(cocoaDat)
describe(cocoaDat)
summary(cocoaDat)
head(cocoaDat)
library(ggplot2)
#Explore the data
ggpairs(cocoaDat)
library(ggplot2)
#Explore the data
ggpairs(cocoaDat)
library(ggally)
library(GGally)
install.packages("GGally")
library(GGally)
#Explore the data
ggpairs(cocoaDat)
#Explore the data
pairs(cocoaDat)
#Explore the data
pairs(cocoaDat[,-1])
X <- cocoaDat[c("shade_tree_cover", "shade_tree_density_ha", "diameter_at_breasthelight")]
X <- cocoaDat[c("shade_tree_cover", "shade_tree_density_ha", "diameter_at_breastheight")]
X <- cocoaDat[c("shade_tree_cover", "shade_tree_density_ha", "diameter_at_breastheight")]
y <- cocoaDat["yield"]
hist(cocoaDat["yield"])
#Explore the data
pairs(cocoaDat[,c(2,5,6,10)])
hist(cocoaDat["yield"])
class(cocoaDat["yield"])
hist(cocoaDat[["yield"]])
?hist
hist(cocoaDat[["yield"]], breaks=20)
hist(cocoaDat[["yield"]], breaks=10)
hist(cocoaDat[["yield"]])
hist(cocoaDat[["yield"]])
hist(cocoaDat[["aboveground_carbon"]])
hist(cocoaDat[["max_temperature"]])
str(cocoaDat)
boxplot(cocoaDat$max_temperature)
m1 <- lm(y~x,data=cocoaDat) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
X <- cocoaDat[c("shade_tree_cover", "shade_tree_density_ha", "diameter_at_breastheight")]
m1 <- lm(y~X,data=cocoaDat) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
X <- cocoaDat[c("shade_tree_cover", "shade_tree_density_ha", "diameter_at_breastheight")]
y <- cocoaDat[["yield"]]
m1 <- lm(y~X,data=cocoaDat) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
m1 <- lm(y~X) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
?lm
m1 <- lm(yield ~ shade_tree_cover, data = cocoaDat) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
par(mfrow=c(1,2))
plot(m1$residuals~m1$fitted, main="residual plot")
abline(h=0)
qqnorm(m1$residuals, main = "qq-plot")
qqline(m1$residuals)
str(m1)
plot(m1$residuals ~ m1$fitted.values)
#yield ~ shade_tree_cover
m1 <- lm(yield ~ shade_tree_cover, data = cocoaDat) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
par(mfrow=c(1,2))
plot(m1$residuals~m1$fitted, main="residual plot")
abline(h=0)
qqnorm(m1$residuals, main = "qq-plot")
qqline(m1$residuals)
summary(m1)
confint(m1)
X <- cocoaDat[c("shade_tree_cover", "shade_tree_density_ha", "diameter_at_breastheight")]
y <- cocoaDat[["yield"]]
#Model
#yield ~ shade_tree_cover
m1 <- lm(y ~ X) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
X <- cocoaDat[c("shade_tree_cover")]
y <- cocoaDat[["yield"]]
#Model
#yield ~ shade_tree_cover
m1 <- lm(y ~ X) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
par(mfrow=c(1,2))
X <- cocoaDat["shade_tree_cover"]
y <- cocoaDat[["yield"]]
#Model
#yield ~ shade_tree_cover
m1 <- lm(y ~ X) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
X <- cocoaDat[["shade_tree_cover"]]
y <- cocoaDat[["yield"]]
#Model
#yield ~ shade_tree_cover
m1 <- lm(y ~ X) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
par(mfrow=c(1,2))
plot(m1$residuals~m1$fitted, main="residual plot")
abline(h=0)
qqnorm(m1$residuals, main = "qq-plot")
qqline(m1$residuals)
summary(m1)
plot(y ~ X)
plot(y ~ X)
plot(y ~ X, xlab = "Shade tree cover", ylab = "Yield")
#Model
#yield ~ shade_tree_cover
m1 <- lm(y ~ X) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
par(mfrow=c(1,2))
plot(m1$residuals~m1$fitted, main="residual plot")
abline(h=0)
qqnorm(m1$residuals, main = "qq-plot")
qqline(m1$residuals)
summary(m1)
confint(m1)
summary(m1)
confint(m1)
#Model
#yield ~ shade_tree_cover
m1 <- lm(cocaoDat["yield"] ~ cocoaDat["shade_tree_cover"]) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
#Model
#yield ~ shade_tree_cover
m1 <- lm(cocoaDat["yield"] ~ cocoaDat["shade_tree_cover"]) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
#Model
#yield ~ shade_tree_cover
m1 <- lm(cocoaDat[["yield"]] ~ cocoaDat[["shade_tree_cover"]]) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
par(mfrow=c(1,2))
plot(m1$residuals~m1$fitted, main="residual plot")
abline(h=0)
qqnorm(m1$residuals, main = "qq-plot")
qqline(m1$residuals)
summary(m1)
confint(m1)
#Model
#yield ~ shade_tree_cover
m1 <- lm(yield ~ shade_tree_cover, data = cocoaDat) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
par(mfrow=c(1,2))
plot(m1$residuals~m1$fitted, main="residual plot")
abline(h=0)
qqnorm(m1$residuals, main = "qq-plot")
qqline(m1$residuals)
summary(m1)
confint(m1)
with(cocoaDat, plot(yield ~ shade_tree_cover, xlab = "Shade tree cover", ylab = "Yield"))
#Model
#yield ~ shade_tree_cover
m1 <- lm(yield ~ shade_tree_cover, data = cocoaDat) # this is the R syntax for fitting a linear model with Gaussian errors. The syntax "height~1", and without the addition of predictor variables, means that we are fitting an 'intercept-only' or constants-only model.
par(mfrow=c(1,2))
plot(m1$residuals~m1$fitted, main="residual plot")
abline(h=0)
qqnorm(m1$residuals, main = "qq-plot")
qqline(m1$residuals)
summary(m1)
confint(m1)
abline(lm(yield ~ shade_tree_cover, data = cocoaDat),col='red')
plot(yield ~ shade_tree_cover, data = cocoaDat, xlab = "Shade tree cover", ylab = "Yield"))
plot(y ~ X, xlab = "Shade tree cover", ylab = "Yield")
abline(lm(yield ~ shade_tree_cover, data = cocoaDat), col = "red")
plot(yield ~ shade_tree_cover, data = cocoaDat, xlab = "Shade tree cover", ylab = "Yield")
abline(lm(yield ~ shade_tree_cover, data = cocoaDat), col = "red")
summary(m1)
confint(m1)
abline(m1, col = "red")
par(mfrow=c(1,2))
plot(m1$residuals~m1$fitted, main="residual plot")
abline(h=0)
qqnorm(m1$residuals, main = "qq-plot")
qqline(m1$residuals)
summary(m1)
confint(m1)
plot(yield ~ shade_tree_cover, data = cocoaDat, xlab = "Shade tree cover", ylab = "Yield")
abline(m1, col = "red")
par(mfrom=c(1,1))
plot(yield ~ shade_tree_cover, data = cocoaDat, xlab = "Shade tree cover", ylab = "Yield")
par(mfrow=c(1,1))
plot(yield ~ shade_tree_cover, data = cocoaDat, xlab = "Shade tree cover", ylab = "Yield")
abline(m1, col = "red")
#Assess assumptions
par(mfrow=c(1,2))
plot(m1$residuals~m1$fitted, main="residual plot")
abline(h=0)
qqnorm(m1$residuals, main = "qq-plot")
qqline(m1$residuals)
par(mfrow=c(2,2))
plot(m1)
m2 <- lm(yield ~ poly(shade_tree_cover, degree = 2), data = cocoaDat)
par(mfrow=c(2,2))
plot(m2)
m2 <- lm(yield ~ poly(shade_tree_cover, degree = 2), data = cocoaDat)
par(mfrow=c(2,2))
plot(m2)
anova(m1,m2)
newdat <- data.frame(shade_tree_cover=seq(min(cocoaDat$shade_tree_cover, na.rm = TRUE),max(cocoaDat$shade_tree_cover, na.rm = TRUE),length.out=50))
head(newdat)
predCI <- as.data.frame(predict(m2,newdata = newdat,interval = "c"))
head(predCI)
plot(yield ~ shade_tree_cover, data = cocoaDat)
par(mfrow=c(1,1))
plot(yield ~ shade_tree_cover, data = cocoaDat)
lines(predCI$fit~newdat$shade_tree_cover,lwd=2) # add fitted/predicted line
lines(predCI$lwr~newdat$shade_tree_cover,lty=3,col="red",lwd=2) # add line for lower bound on CI
lines(predCI$upr~newdat$shade_tree_cover,lty=3,col="red",lwd=2) # add line for upper bound on CI
diffs <- data.frame(method = c("deSolve", paste0("Euler_", stepSize)),
difference = NA)
### 2.1.c
stepSize <- c(5, 1, 0.1, 0.001)
diffs <- data.frame(method = c("deSolve", paste0("Euler_", stepSize)),
difference = NA)
View(diffs)
diffs$difference[1] <- abs(solutionNum[nrow(solutionNum), "N"] -
solutionAna[nrow(solutionNum)])
### 2.1.a
Nsolution <- function(t, N0, K, r) {
N0*K*exp(r*t)/(K+N0*(exp(r*t) - 1))
}
timesAna <- seq(0, 100, by=0.1)
parameters <- c(r=0.2, K=1000)
iniState <- c(N=1)
solutionAna <- solution(timesAna, iniState, parameters["K"], parameters["r"])
plot(x = timesAna, y = solutionAna, xlab = "Time", ylab = "Population size", type = "l", main = "")
### 2.1.a
solution <- function(t, N0, K, r) {
N0*K*exp(r*t)/(K+N0*(exp(r*t) - 1))
}
timesAna <- seq(0, 100, by=0.1)
parameters <- c(r=0.2, K=1000)
iniState <- c(N=1)
solutionAna <- solution(timesAna, iniState, parameters["K"], parameters["r"])
plot(x = timesAna, y = solutionAna, xlab = "Time", ylab = "Population size", type = "l", main = "")
### 2.1.b
logisticODE <- function(times, state, parameters) {
with(as.list(c(state, parameters)), {
dN<-r*N*(1-N/K)
return(list(c(dN)))
})
}
timesNum <- seq(0, 100, by=0.1)
solutionNum<-ode(iniState, timesNum, logisticODE, parameters)
plot(x = timesAna, y = solutionAna, xlab = "Time", ylab = "Population size", type = "l", main = "")
lines(x = timesNum, y = solutionNum[,"N"], col = "red")
### 2.1.c
stepSize <- c(5, 1, 0.1, 0.001)
diffs <- data.frame(method = c("deSolve", paste0("Euler_", stepSize)),
difference = NA)
diffs$difference[1] <- abs(solutionNum[nrow(solutionNum), "N"] -
solutionAna[nrow(solutionNum)])
### 2.1.b
logisticODE <- function(times, state, parameters) {
with(as.list(c(state, parameters)), {
dN<-r*N*(1-N/K)
return(list(c(dN)))
})
}
timesNum <- seq(0, 100, by=0.1)
solutionNum<-ode(iniState, timesNum, logisticODE, parameters)
plot(x = timesAna, y = solutionAna, xlab = "Time", ylab = "Population size", type = "l", main = "")
library(deSolve)
### 2.1.b
logisticODE <- function(times, state, parameters) {
with(as.list(c(state, parameters)), {
dN<-r*N*(1-N/K)
return(list(c(dN)))
})
}
timesNum <- seq(0, 100, by=0.1)
solutionNum<-ode(iniState, timesNum, logisticODE, parameters)
plot(x = timesAna, y = solutionAna, xlab = "Time", ylab = "Population size", type = "l", main = "")
lines(x = timesNum, y = solutionNum[,"N"], col = "red")
### 2.1.c
stepSize <- c(5, 1, 0.1, 0.001)
diffs <- data.frame(method = c("deSolve", paste0("Euler_", stepSize)),
difference = NA)
diffs$difference[1] <- abs(solutionNum[nrow(solutionNum), "N"] -
solutionAna[nrow(solutionNum)])
solutionNum[nrow(solutionNum), "N"]
solutionAna[nrow(solutionNum)]
?abs
solutionNum[nrow(solutionNum), "N"]
View(solutionNum)
abs(solutionNum[nrow(solutionNum), "N"] -
solutionAna[nrow(solutionNum)])
nrow(solutionNum)
tail(solutionNum)
solutionNum[nrow(solutionNum), "N"]
solutionAna[nrow(solutionNum)]
for(k in 1:length(solutionEuler)) {
diffs$difference[k + 1] <- abs(EulerFunction[[k]]$N[nrow(EulerFunction[[k]])] -
solutionAna[nrow(solutionNum)])
}
### 2.1.c
stepSize <- c(5, 1, 0.1, 0.001)
solutionEuler <- vector(mode = "list", length = length(stepSize))
View(solutionEuler)
plot(x = timesAna, y = solutionAna, xlab = "Time", ylab = "Population size", type = "l", main = "")
for(k in 1:length(solutionEuler)) {
solutionEuler[[k]] <- data.frame(times = seq(0, 100, by = stepSize[k]), N = NA)
solutionEuler[[k]]$N[1] <- iniState
for(i in 1:(nrow(solutionEuler[[k]])-1)) {
currentN <- c(N = solutionEuler[[k]]$N[i])
solutionEuler[[k]]$N[i + 1] <- currentN +
stepSize[k]*logisticODE(NA, currentN, parameters)[[1]]
}
lines(x = solutionEuler[[k]]$times, y = solutionEuler[[k]]$N, col = "blue")
}
diffs <- data.frame(method = c("deSolve", paste0("Euler_", stepSize)),
difference = NA)
diffs$difference[1] <- abs(solutionNum[nrow(solutionNum), "N"] -
solutionAna[nrow(solutionNum)])
for(k in 1:length(solutionEuler)) {
diffs$difference[k + 1] <- abs(EulerFunction[[k]]$N[nrow(EulerFunction[[k]])] -
solutionAna[nrow(solutionNum)])
}
diffs <- data.frame(method = c("deSolve", paste0("Euler_", stepSize)),
difference = NA)
diffs$difference[1] <- abs(solutionNum[nrow(solutionNum), "N"] -
solutionAna[nrow(solutionNum)])
for(k in 1:length(solutionEuler)) {
diffs$difference[k + 1] <- abs(logisticODE[[k]]$N[nrow(logisticODE[[k]])] -
solutionAna[nrow(solutionNum)])
}
diffs <- data.frame(method = c("deSolve", paste0("Euler_", stepSize)),
difference = NA)
diffs$difference[1] <- abs(solutionNum[nrow(solutionNum), "N"] -
solutionAna[nrow(solutionNum)])
for(k in 1:length(solutionEuler)) {
diffs$difference[k + 1] <- abs(solutionEuler[[k]]$N[nrow(solutionEuler[[k]])] -
solutionAna[nrow(solutionNum)])
}
barplot(diffs$difference, names.arg = diffs$method, log = "y")
box()
View(diffs)
### Task 2.2.a
siModel_a <- function(times, state, parameters) {
with(as.list(c(state, parameters)), {
dS <- -beta*S*I
dI <- beta*S*I - gamma*I
dR <- gamma*I
return(list(c(dS, dI, dR)))
})
}
# Set time points
times <- seq(0, 300, by = 1)
# Set initial conditions and parameters
parameters <- c(beta = 0.3, gamma = 0.1)
iniState <- c(S = 0.99, I = 0.1, R = 0)
siSolution_a <- ode(iniState, times, siModel_a, parameters)
plot(x = siSolution_a[,"time"], y = siSolution_a[,"S"], col="blue", type="l", ylab="y", xlab ='Time', main = "Task 2.2.a")
lines(x = siSolution_a[,"time"], y = siSolution_a[,"I"], col="red")
lines(x = siSolution_a[,"time"], y = siSolution_a[,"R"], col="orange")
# Add a legend
legend("topright", legend = c("S", "I", "R"),
col = c("red", "blue", "orange"), lty = 1)
